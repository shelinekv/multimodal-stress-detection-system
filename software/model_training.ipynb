{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"12ERTimdYbeB2nA-0xWZ2K105hmVLd-Pd","authorship_tag":"ABX9TyODG7q68OjQX3AviZExv48C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"id":"EZ3trWhJtVUe","executionInfo":{"status":"error","timestamp":1734340885870,"user_tz":-330,"elapsed":535,"user":{"displayName":"sheline K V","userId":"14611107045263504625"}},"outputId":"3d2e5e9c-a51e-463d-a4f6-847de36a356d"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'imgg' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-825e396d242c>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mimgg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpicture_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/CK+48/anger/S010_004_00000017.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'imgg' is not defined"]}],"source":["import cv2\n","from google.colab.patches import cv2_imshow\n","import shutil\n","\n","\n","from google.colab import files\n","from IPython.display import Image\n","def picture_crop(pic_name):\n","\n","  data=[]\n","  #desc = LocalBinaryPatterns(24, 8)\n","\n","  #Reading the image img as 3-dimensional metrics\n","  img=cv2.imread(pic_name,1)\n","\n","  #converting the RGB image into gray-scale image.\n","  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","  #enhancing the image by increasing contrast to the objects in the image.\n","  enhanced=cv2.equalizeHist(gray)\n","\n","  #making the object of cascade classifier to  detecting the facial features from the image.\n","  face_cascade=cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n","\n","  #extracting all the coordinates of the faces present in the image.\n","  faces=face_cascade.detectMultiScale(enhanced , 1.23,5)\n","\n","  #making a rectangle to the detected image\n","  for x,y,w,h in faces:\n","    img=cv2.rectangle(enhanced,(x,y),(x+w,y+h),(0,255,0),3)\n","\n","\n","    #extracting the face detected face into another 3D metrics.\n","    subface=img[y:y+h,x:x+w]\n","\n","    #reading the new subface metrics and converting into a new croped image (croped.png)\n","    cv2.imwrite(\"croped.png\",subface)\n","    final_img=cv2.imread('croped.png',0)\n","\n","\n","  #cv2_imshow(data[0])\n","  return final_img\n","  imgg=picture_crop('/content/drive/MyDrive/Colab Notebooks/CK+48/anger/S010_004_00000017.png')\n","cv2_imshow(imgg)\n","from zipfile import ZipFile\n","\n","print(\"Zip file Extracting...  \")\n","\n","emot_zip=['anger.zip','fear.zip','happiness.zip','sadness.zip','contempt.zip','disgust.zip','surprise.zip','neutral.zip']\n","\n","for j in range(len(emot_zip)):\n","  fname=emot_zip[j]\n","  with ZipFile(fname,'r') as zip:\n","    zip.extractall()\n","\n","print(\"done\")\n","#importing opencv and glob libraries\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from glob import glob\n","import numpy as np\n","\n","#initializing Numpy arrays\n","img_pixels=np.zeros((350,48,48))\n","labels=np.zeros((350,))\n","\n","i=0\n","emotions=['anger','fear','happiness','sadness','contempt','disgust','surprise','neutral']\n","print(\"Number of classes\",len(emotions))\n","\n","for j in range(len(emotions)):\n","\n","  #Folder wise data labelling\n","  print(\"Extracting.. \",emotions[j])\n","  data_db=emotions[j]\n","  img_dbb=glob(data_db+'/*.png')\n","\n","  for k in img_dbb:\n","\n","    croped_img=picture_crop(k)\n","    resized=cv2.resize(croped_img,(48,48))\n","    img_pixels[i]=resized\n","    labels[i]=j\n","    i+=1\n","\n","print(\"Total labelled Images  \",i)\n","from sklearn.model_selection import train_test_split\n","\n","train_samples,test_samples,train_labels,test_labels=train_test_split(img_pixels,labels,test_size=0.10,random_state=1)\n","print((train_samples.shape))\n","print((test_samples.shape))\n","print(train_labels.shape)\n","print(test_labels.shape)\n","def convert_dtype(x):\n","\n","    x_float = x.astype('float32')\n","    return x_float\n","\n","train_samples = convert_dtype(train_samples)\n","test_samples = convert_dtype(test_samples)\n","print(train_samples.shape)\n","print(test_samples.shape)\n","def normalize(x):\n","\n","    x_n=x/255\n","    return x_n\n","\n","train_samples = normalize(train_samples)\n","test_samples = normalize(test_samples)\n","print(train_samples.shape)\n","print(test_samples.shape)\n","def oneHot(y, Ny):\n","\n","    from keras.utils import to_categorical\n","    y_oh=to_categorical(y,Ny)\n","    return y_oh\n","\n","\n","train_labels = oneHot(train_labels, 8)\n","test_labels = oneHot(test_labels, 8)\n","print(train_labels.shape)\n","print(test_labels.shape)\n","def reshape(x):\n","\n","    print(x.shape)\n","    x_r=x.reshape(x.shape[0], x.shape[1], x.shape[2], 1)\n","    return x_r\n","\n","train_samples = reshape(train_samples)\n","print(train_samples.shape)\n","\n","test_samples = reshape(test_samples)\n","print(test_samples.shape)\n",""]},{"cell_type":"code","source":["import cv2\n","from google.colab.patches import cv2_imshow\n","\n","def picture_crop(pic_name):\n","    # Reading the image as a 3-dimensional matrix\n","    img = cv2.imread(pic_name, 1)\n","\n","    if img is None:\n","        print(f\"Error: Image not found at path {pic_name}\")\n","        return None\n","\n","    # Converting the RGB image to grayscale\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","    # Enhancing the image by increasing contrast\n","    enhanced = cv2.equalizeHist(gray)\n","\n","    # Initializing the Cascade Classifier for face detection\n","    face_cascade = cv2.CascadeClassifier(\"/content/drive/MyDrive/Colab Notebooks/CK+48/haarcascade_frontalface_default.xml\")\n","\n","    # Detecting faces in the image\n","    faces = face_cascade.detectMultiScale(enhanced, 1.23, 5)\n","\n","    if len(faces) == 0:\n","        print(\"No face detected in the image.\")\n","        return None\n","\n","    # Loop through detected faces\n","    for x, y, w, h in faces:\n","        subface = enhanced[y:y+h, x:x+w]  # Crop the face region\n","        resized = cv2.resize(subface, (48, 48))  # Resize the cropped face to 48x48\n","        return resized\n","\n","    return None\n","\n","# Test the function\n","imgg = picture_crop('/content/drive/MyDrive/Colab Notebooks/CK+48/anger/S010_004_00000017.png')\n","\n","if imgg is not None:\n","    cv2_imshow(imgg)  # Display the cropped image\n","else:\n","    print(\"Failed to process the image.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYjrEfl1yDsF","executionInfo":{"status":"ok","timestamp":1734341225351,"user_tz":-330,"elapsed":541,"user":{"displayName":"sheline K V","userId":"14611107045263504625"}},"outputId":"210e9158-1d1a-4a00-de67-5b2e2a5d6b71"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["No face detected in the image.\n","Failed to process the image.\n"]}]}]}